---
title: "Foundations of Probability and Statistics, Project"
author: "Emiliano Capasso, Antonello Scarcella, Simone Bellavia"
date: '2022-12-05'
geometry: "left=2cm,right=2cm,top=0.5cm,bottom=1.5cm"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE,
                      message=FALSE,
                      tidy.opts=list(width.cutoff = 80),
                      tidy = TRUE)
library(tidyverse) # for tidyverse
library(caret) # for createDataPartition
library(performance) #Â for compare_performance
library(ggplot2) # for plots
library(reshape2) # for melt function
library(gridExtra) # to arrange plots in a grid 
library(factoextra) # for PCA
library(corrplot) # for correlation between variables and PCA
```

# Introduction to Analysis

TBD

# Descriptive Analysis

TBD

## Importing data

```{r}
# import data
data <- read.csv("data.csv",
                    header = TRUE)
```

## First exploration

```{r}
# get structure of dataset
str(data)
```

- id: id number
- diagnosis (response): the diagnosis of breast tissues (M = malignant, B = benign)
- radius_mean: mean of distances from center to points on the perimeter
- texture_mean: standard deviation of gray-scale values
- perimeter_mean: mean size of the core tumor
- area_mean: //
- smoothness_mean: mean of local variation in radius lengths
- compactness_mean: mean of perimeter^2 / area - 1.0
- concavity_mean: mean of severity of concave portions of the contour
- concave points_mean: mean for number of concave portions of the contour
- ecc.

```{r}
# get summary of variables
summary(data)
```

```{r}
# getting the head of dataset
head(data)
```

## Missing Values

Checking if there are missing values:

```{r}
# check for missing values
colSums(is.na(data))
```

There aren't missing values in the considered dataset, except for 32th feature 'X' that is full of NA, we drop it.

```{r}
data <- data %>% select(-X)
```

We also drop id because it is not needed

```{r}
data <- data %>% select(-id)
```

Checking if the column has been dropped.

```{r}
colnames(data)
```

## Second exploration

```{r}
#describe(data)
```

Let's see the frequency of cancer diagnosis (if benignant or malignant)

```{r}
ggplot(data, aes(x = diagnosis)) + 
  geom_bar(aes(y = (after_stat(count))/sum(after_stat(count)))) + 
  scale_fill_manual(values = c("#0468BF","#D9A23D")) + 
theme_bw() +
  labs(x = "Diagnosis", y = "Frequency", title = "Frequency of Cancer Stages")
```

Benignant cancers are way more than malignant.

Converting the diagnosis values in numeric. Malignant == 1, Benign == 0.

```{r}
data$diagnosis <- as.numeric(data$diagnosis == "M")

str(data)
```

Let's see the correlation map with a heatmap.

```{r}
# Calculate the correlation matrix
corr_matrix <- cor(data)

# Melt the correlation matrix into a data frame
melted_matrix <- melt(corr_matrix)

# Add the correlation coefficient as a label to each tile
melted_matrix$label <- round(melted_matrix$value, 2)

# Plot the heatmap using ggplot2
ggplot(data = melted_matrix, aes(x = Var1, y = Var2, fill = value)) + 
  geom_tile(color = "white") +
  geom_text(aes(label = label), color = "black", size = 3) + 
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0,
    limit = c(-1, 1), name = "Correlation") + 
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  ggtitle("Correlation Map")
```


Features vs Diagnosis

```{r}
features_mean <- names(data)[2:11]

plots <- lapply(1:length(features_mean), function(x) {
  g <- ggplot(data, aes_string(x = features_mean[x], 
                              fill = as.factor(data$diagnosis))) +
    geom_histogram(binwidth = (max(data[,features_mean[x]]) - min(data[,features_mean[x]]))/50, 
                  alpha = 0.5, aes(color = as.factor(data$diagnosis))) +
    scale_fill_manual(values = c("green", "red")) +
    scale_color_manual(values = c("green", "red")) +
    ggtitle(features_mean[x]) +
    theme(plot.title = element_text(hjust = 0.5)) +
    labs(fill = "Diagnosis", color = "Diagnosis")
  return(g)
})

grid.arrange(grobs = plots, ncol = 2, widths = c(60, 60), height = 60)
```

From the heatmap we analyse the features against the response variable diagnosis,
and we can see the most correlated are those with a value higher than 0.6.
So we can discard: 
- all the variables "*_se"
- texture_*
- smoothness_*
- symmerty_*
- fractal_dimension_*

on the remains we can conduct more in-depth analysis

This code plots histograms of 10 features of a dataset with a binary diagnosis outcome. 
The histograms are stacked and show the distribution of each feature for each diagnosis type (red for "M" and green for "B"). 
The histograms are plotted in a 5x2 grid, with each feature in a separate plot and labeled with the feature name. 
The code uses ggplot2 library in R for plotting and gridExtra library for arranging the plots in a grid.
The resulting plot is a comparison of the distributions of each feature between the two diagnosis groups.

Mean values of texture, smoothness, symmetry or fractual dimension does not show a particular preference of one diagnosis over the other. 

## Feature Selection

### Covariance and Correlation

followed the analysis above we can explore more the variables left.

```{r}
cov(data[c("radius_mean","perimeter_mean","area_mean","radius_worst",
"perimeter_worst","area_worst","diagnosis")])

cor(data[c("radius_mean","perimeter_mean","area_mean","radius_worst",
"perimeter_worst","area_worst","diagnosis")])

pairs(data[c("radius_mean","perimeter_mean","area_mean","radius_worst",
"perimeter_worst","area_worst")])
```

from the plot and the correlation values we can see a very strong correlation between all the features
in this table so we can drop them all except for one. We select the feature which has the higher correlation with the response variable diagnosis
so we select the perimeter_worst with a value of 0.783.
There is also a strong correlation between the radius_worst, perimeter_worst and area_worst.

we now test the last remained variables

```{r}
cov(data[c("concavity_mean","compactness_mean","concave.points_mean",
"concavity_worst","compactness_worst","concave.points_worst")])

cor(data[c("concavity_mean","compactness_mean","concave.points_mean",
"concavity_worst","compactness_worst","concave.points_worst","diagnosis")])

pairs(data[c("concavity_mean","compactness_mean","concave.points_mean",
"concavity_worst","compactness_worst","concave.points_worst")])
```

from the analysis and the plot we can see that concave.point_worst and concave.points_mean
are strongly correlated so we keep only concave.point_worst which has the higher
correlation with diagnosis.
Same goes for concavity and compacteness mean with they respective worst have a correlation
value less than 0.9 but still strongly correlated.
We keep concavity_worst and compactness_mean whose have the higher correlation
with the diagnosis (0.659,0.696)

as a summary we plot the correlation matrix of selected features
```{r}
cor(data[c("perimeter_worst","concavity_worst","compactness_mean","concave.points_worst")])

pairs(data[c("perimeter_worst","concavity_worst","compactness_mean","concave.points_worst")])
```

# Tests

## Chi-sq Test

for further analysis we want to test the indipendence of the variable 
where the correlation values is in the range of 0.8-0.9
in order to do this we use the Chi-squared test

H0: The two variables are independent.
H1: The two variables relate to each other.

```{r}
chisq.test(data$concavity_worst,data$compactness_mean, correct = FALSE)
```

p value is > 0.05 so we accept the null hypotesis, the two variable are indipendent

```{r}
chisq.test(data$concave.points_worst,data$perimeter_worst, correct = FALSE)
```
p value is > 0.05 so we accept the null hypotesis, the two variable are indipendent
```{r}
chisq.test(data$concave.points_worst,data$concavity_worst, correct = FALSE)
```
p value is < 0.05 so we reject the null hypotesis, the two variable are dependant

we want to estimate the difference between the mean of radius size of all benign and all malignant
tumor to understand if the difference is real of given by sampling error. To do so, we draw two samples, one from the population of malignant tumor and another
from the population of benign tumor. These two samples are independent because they
are drawn from two different populations, and the samples have no effect on each other.

TBD

# Linear Model

TBD

# Bibliography

[1] TBD